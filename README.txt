## Project Setup
1. Clone or download https://github.com/lvwf1/CS7641_MachineLearningHW4 repository.
2. Download and install Eclipse Java IDE.
3. Import the project inside of Eclipse.
4. When the project is imported, right click on the top-level directory shown on the left.
5. Navigate through context menu to Maven, then click Update Project. This should resolve classpath issues.
Note: if you're having issues related to the classpath, or import/dependency issues try to repeat steps 4 and 5. 

## Running the Grid World: Low Difficulty Analysis:

1. While inside the directory structure CS7641_MachineLearningHW4/src/main/java/assignment4/ right-click on the EasyGridWorldLauncher.
2. Go to the “Run As…” section and select "Java Application".
3. All three algorithms will run and the aggregate analysis and optimal policies will be printed to the console.

## Running the Grid World: High Difficulty Analysis:

1. While inside the directory structure CS7641_MachineLearningHW4/src/main/java/assignment4/ right-click on the HardGridWorldLauncher.
2. Go to the “Run As…”n section and select "Java Application".
3. All three algorithms will run and the aggregate analysis and optimal policies will be printed to the console.

## Sample Output
This is the sort of output you get out of the box by running the HardGridWorldLauncher as a Java Application:

```
/////Easy Grid World Analysis/////

This is your grid world:
{ 0, 0, 1, 0, 0},
{ 1, 0, 1, 0, 1},
{ 0, 0, 0, 0, 0},
{ 0, 1, 1, 1, 0},
{ 0, 0, 0, 0, 0}

//Aggregate Analysis//

The data below shows the number of steps/actions the agent required to reach 
the terminal state given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,18,217,10,12,17,11,11,11,11,9,10,11,11,13,13,9,13,11,16,12,11,13,9,16,9,20,17,9,11,10,12,9,11,21,9,15,11,11,9,13,9,11,11,13,9,11,10,13,17,11,10,12,14,14,18,13,11,12,15,10,15,12,9,13,9,11,15,11,10,10,11,10,15,10,10,9,12,13,11,21,10,13,10,10,11,9,11,10,12,15,20,19,13,15,9,9,13,11,9,10
Policy Iteration,772,95,14,11,12,13,16,9,10,10,12,9,9,12,10,14,13,9,9,10,9,12,11,17,11,10,9,9,11,10,9,10,17,10,10,14,10,10,9,13,11,15,9,15,14,13,13,9,15,12,12,9,12,12,16,13,11,14,11,9,23,18,10,10,9,10,11,14,11,13,9,9,12,12,12,11,11,9,15,12,11,16,14,12,12,13,16,11,11,13,10,11,10,9,13,10,12,13,10,12
Q Learning,118,255,197,33,28,74,26,28,14,24,18,28,19,12,49,11,19,15,32,18,15,17,25,10,44,39,14,15,11,28,24,13,17,18,14,11,31,20,19,34,38,12,11,36,10,17,22,42,25,12,13,38,10,26,16,12,9,37,18,12,19,17,18,14,20,43,21,12,13,44,23,25,13,13,48,17,13,20,11,9,17,25,13,9,15,17,10,13,21,18,31,39,17,14,25,20,56,28,12,42

The data below shows the number of milliseconds the algorithm required to generate 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration,36,9,4,6,6,6,7,5,6,5,7,7,5,5,4,8,12,6,6,9,9,5,8,20,21,21,16,13,17,7,7,8,11,11,8,9,18,9,14,14,13,15,17,16,15,19,17,17,19,22,21,15,11,8,8,8,8,12,8,9,9,9,15,9,10,10,9,9,13,11,10,11,10,22,22,23,21,19,18,17,20,21,12,12,13,14,13,13,12,13,22,19,20,14,14,15,16,19,14,17
Policy Iteration,7,2,5,3,7,9,3,6,6,9,6,5,9,7,8,6,10,8,9,11,9,9,13,12,17,23,18,23,20,17,17,18,18,23,21,23,26,26,28,31,15,14,14,17,17,16,25,17,19,18,19,17,17,28,27,25,20,26,27,27,31,28,31,31,36,30,25,24,23,25,25,25,24,26,25,39,55,47,49,27,28,30,29,29,31,29,37,29,29,30,37,34,33,42,43,35,51,40,49,40
Q Learning,22,15,28,15,36,7,9,7,2,2,2,4,4,4,8,4,3,7,4,3,6,6,4,2,3,5,2,5,5,4,5,5,8,4,4,3,6,4,5,7,5,4,5,5,4,5,4,5,7,7,10,4,7,7,6,6,9,7,11,7,9,8,33,15,8,8,6,4,5,6,5,5,5,6,4,5,7,5,8,5,4,5,4,4,4,5,6,6,5,7,5,5,5,4,5,6,7,6,7,5

The data below shows the total reward gained for 
the optimal policy given the number of iterations the algorithm was run.
Iterations,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100
Value Iteration Rewards,84.0,-115.0,92.0,90.0,85.0,91.0,91.0,91.0,91.0,93.0,92.0,91.0,91.0,89.0,89.0,93.0,89.0,91.0,86.0,90.0,91.0,89.0,93.0,86.0,93.0,82.0,85.0,93.0,91.0,92.0,90.0,93.0,91.0,81.0,93.0,87.0,91.0,91.0,93.0,89.0,93.0,91.0,91.0,89.0,93.0,91.0,92.0,89.0,85.0,91.0,92.0,90.0,88.0,88.0,84.0,89.0,91.0,90.0,87.0,92.0,87.0,90.0,93.0,89.0,93.0,91.0,87.0,91.0,92.0,92.0,91.0,92.0,87.0,92.0,92.0,93.0,90.0,89.0,91.0,81.0,92.0,89.0,92.0,92.0,91.0,93.0,91.0,92.0,90.0,87.0,82.0,83.0,89.0,87.0,93.0,93.0,89.0,91.0,93.0,92.0
Policy Iteration Rewards,-670.0,7.0,88.0,91.0,90.0,89.0,86.0,93.0,92.0,92.0,90.0,93.0,93.0,90.0,92.0,88.0,89.0,93.0,93.0,92.0,93.0,90.0,91.0,85.0,91.0,92.0,93.0,93.0,91.0,92.0,93.0,92.0,85.0,92.0,92.0,88.0,92.0,92.0,93.0,89.0,91.0,87.0,93.0,87.0,88.0,89.0,89.0,93.0,87.0,90.0,90.0,93.0,90.0,90.0,86.0,89.0,91.0,88.0,91.0,93.0,79.0,84.0,92.0,92.0,93.0,92.0,91.0,88.0,91.0,89.0,93.0,93.0,90.0,90.0,90.0,91.0,91.0,93.0,87.0,90.0,91.0,86.0,88.0,90.0,90.0,89.0,86.0,91.0,91.0,89.0,92.0,91.0,92.0,93.0,89.0,92.0,90.0,89.0,92.0,90.0
Q Learning Rewards,-16.0,-153.0,-95.0,69.0,74.0,28.0,76.0,74.0,88.0,78.0,84.0,74.0,83.0,90.0,53.0,91.0,83.0,87.0,70.0,84.0,87.0,85.0,77.0,92.0,58.0,63.0,88.0,87.0,91.0,74.0,78.0,89.0,85.0,84.0,88.0,91.0,71.0,82.0,83.0,68.0,64.0,90.0,91.0,66.0,92.0,85.0,80.0,60.0,77.0,90.0,89.0,64.0,92.0,76.0,86.0,90.0,93.0,65.0,84.0,90.0,83.0,85.0,84.0,88.0,82.0,59.0,81.0,90.0,89.0,58.0,79.0,77.0,89.0,89.0,54.0,85.0,89.0,82.0,91.0,93.0,85.0,77.0,89.0,93.0,87.0,85.0,92.0,89.0,81.0,84.0,71.0,63.0,85.0,88.0,77.0,82.0,46.0,74.0,90.0,60.0

```
/////Hard Grid World Analysis/////

This is your grid world:
{ 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0},
{ 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0},
{ 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0},
{ 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1},
{ 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1},
{ 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1},
{ 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1},
{ 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1},
{ 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1},
{ 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0},
{ 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0},
{ 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0},
{ 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1},
{ 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1},
{ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1}

## Run analysis
Import analysis.py into Pycharm project, install matplotlib and numpy package into virtual environment, click run at the top menu.

The Java Code is modified from https://github.com/juanjose49/omscs-cs7641-machine-learning-assignment-4
